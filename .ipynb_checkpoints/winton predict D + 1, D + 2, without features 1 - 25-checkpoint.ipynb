{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from winton_script_ver1 import load_data, get_xy, get_cols, pre_process, hyper_parameter_search\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor # used for transforming the target variable\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Loading data ...\n"
     ]
    }
   ],
   "source": [
    "train, test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, ytrain = get_xy(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 columns contained the search word Feature \n"
     ]
    }
   ],
   "source": [
    "y1 = ytrain['Ret_PlusOne'] # D + 1 returns\n",
    "y2 = ytrain['Ret_PlusTwo'] # D + 2 returns\n",
    "cols = get_cols(xtrain)\n",
    "xtrain = xtrain.iloc[:, cols: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain1, xtest1, ytrain1, ytest1 = pre_process(xtrain, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction D + 1\n",
    "\n",
    "$R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$\n",
    "\n",
    "\n",
    "\n",
    "$explained\\_{}variance(y, \\hat{y}) = 1 - \\frac{Var\\{ y - \\hat{y}\\}}{Var\\{y\\}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\"max_depth\": sp_randint(2, 10), \n",
    "              \"max_features\": ['auto', 'sqrt', 'log2', None], \n",
    "              \"min_samples_split\": sp_randint(2, 50), \n",
    "              'min_samples_leaf': sp_randint(2, 500),\n",
    "              \"n_estimators\": sp_randint(200, 300), \n",
    "              'loss': ['huber', 'lad', 'lad'], \n",
    "              'learning_rate': [0.1, 0.2, 0.3, 0.0001], \n",
    "              'n_iter_no_change': sp_randint(1, 50)}\n",
    "\n",
    "train_samples = 20000\n",
    "test_samples = 5000\n",
    "n_iter = 50\n",
    "model = GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model used: GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, n_iter_no_change=None, presort='auto',\n",
      "             random_state=None, subsample=1.0, tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   24.9s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   32.5s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   42.4s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed: 15.4min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 19.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with training.\n",
      "R2 score train set: 0.3581 score test set: 0.0577\n",
      "Mean-squared error train set: 0.0004 test set: 0.0006\n",
      "Explained variance train set: 0.3581 test set: 0.058\n",
      "Root mean squared error train set 0.0195 test set: 0.0236\n",
      "..................................................\n",
      "Best parameters:\n",
      "{'learning_rate': 0.2, 'loss': 'huber', 'max_depth': 8, 'max_features': None, 'min_samples_leaf': 47, 'min_samples_split': 17, 'n_estimators': 281, 'n_iter_no_change': 44}\n"
     ]
    }
   ],
   "source": [
    "reg, params = hyper_parameter_search(xtrain1, \n",
    "                                     ytrain1, \n",
    "                                     xtest1, \n",
    "                                     ytest1, \n",
    "                                     model=model,\n",
    "                                     param_dist=param_dist,\n",
    "                                     n_iter_search=n_iter,\n",
    "                                     train_samples=train_samples,\n",
    "                                     test_samples=test_samples,\n",
    "                                     cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction D + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain2, xtest2, ytrain2, ytest2 = pre_process(xtrain, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model used: GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, n_iter_no_change=None, presort='auto',\n",
      "             random_state=None, subsample=1.0, tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 12.4min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed: 13.1min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 15.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with training.\n",
      "R2 score train set: 0.3094 score test set: 0.0398\n",
      "Mean-squared error train set: 0.0004 test set: 0.0006\n",
      "Explained variance train set: 0.3095 test set: 0.0399\n",
      "Root mean squared error train set 0.0201 test set: 0.0238\n",
      "..................................................\n",
      "Best parameters:\n",
      "{'learning_rate': 0.3, 'loss': 'lad', 'max_depth': 8, 'max_features': None, 'min_samples_leaf': 19, 'min_samples_split': 4, 'n_estimators': 255, 'n_iter_no_change': 20}\n"
     ]
    }
   ],
   "source": [
    "reg, params = hyper_parameter_search(xtrain2, \n",
    "                                     ytrain2,\n",
    "                                     xtest2, \n",
    "                                     ytest2,\n",
    "                                     model=model,\n",
    "                                     param_dist=param_dist,\n",
    "                                     n_iter_search=n_iter,\n",
    "                                     train_samples=train_samples,\n",
    "                                     test_samples=test_samples,\n",
    "                                     cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
